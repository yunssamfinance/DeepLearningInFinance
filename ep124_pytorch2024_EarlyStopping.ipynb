{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Episode 124. Early Stopping (ver. 2024)"
      ],
      "metadata": {
        "id": "-RmUUAOnyCt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is a technique that helps prevent overfitting and optimize model performance by monitoring validation loss during training. We can avoid unnecessary iterations and save computational resources by stopping the training process when the validation loss starts increasing. Additionally, we will explore how to save and load PyTorch networks, allowing us to store trained models and reuse them for predictions or further training."
      ],
      "metadata": {
        "id": "fhFxLHCdyncu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Google CoLab is running and maps Google Drive if needed. We also initialize the PyTorch device to either GPU/MPS (if available) or CPU.\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
        "import torch\n",
        "has_mps = torch.backends.mps.is_built()\n",
        "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP-A7eQjyC5N",
        "outputId": "f4fa98de-c6d1-4834-eec9-ed0b6c1b0b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping\n",
        "\n",
        "It can be difficult to determine how many epochs to cycle through to train a neural network. Overfitting will occur if you train the neural network for too many epochs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize."
      ],
      "metadata": {
        "id": "qrFvqtOgyrUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to segment the original dataset into several datasets:\n",
        "\n",
        "- **Training Set**\n",
        "- **Validation Set**\n",
        "- **Holdout Set**\n"
      ],
      "metadata": {
        "id": "qqZxDuFZyz-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch does not include a built-in early stopping function. Here we define one.\n",
        "\n",
        "We need to provide several parameters to the **EarlyStopping** object:\n",
        "\n",
        "- **min_delta** This value should be kept small; it specifies the minimum change that should be considered an improvement. Setting it even smaller will not likely have a great deal of impact.\n",
        "- **patience** How long should the training wait for the validation error to improve?\n",
        "- **restore_best_weights** You should usually set this to true, as it restores the weights to the values they were at when the validation set is the highest."
      ],
      "metadata": {
        "id": "b3sWmJCdy4H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_model = None\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.status = \"\"\n",
        "\n",
        "    def __call__(self, model, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "        elif self.best_loss - val_loss >= self.min_delta:\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
        "            if self.counter >= self.patience:\n",
        "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
        "                if self.restore_best_weights:\n",
        "                    model.load_state_dict(self.best_model)\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "sy5p6QEUyC8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1: Early Stopping with Classification"
      ],
      "metadata": {
        "id": "6MCsH_Wj0xm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def load_data():\n",
        "    df = pd.read_csv(\n",
        "        \"https://raw.githubusercontent.com/yunssamfinance/DeepLearningInFinance/main/Default_Fin.csv\", na_values=[\"NA\", \"?\"]\n",
        "    )\n",
        "\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    x = df[[\"Employed\", \"Bank Balance\", \"Annual Salary\"]].values\n",
        "    y = le.fit_transform(df[\"Defaulted?\"])\n",
        "    defaulted = le.classes_\n",
        "\n",
        "    # Split into validation and training sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=0.25, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    # Numpy to Torch Tensor\n",
        "    x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n",
        "\n",
        "    x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test, defaulted\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test, defaulted = load_data()\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset_train = TensorDataset(x_train, y_train)\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_test, y_test)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Create model using nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 25),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(25, len(defaulted)),\n",
        "    nn.LogSoftmax(dim=1),\n",
        ")\n",
        "\n",
        "model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()  # cross entropy loss\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "es = EarlyStopping()\n",
        "\n",
        "epoch = 0\n",
        "done = False\n",
        "while epoch < 1000 and not done:\n",
        "    epoch += 1\n",
        "    steps = list(enumerate(dataloader_train))\n",
        "    pbar = tqdm.tqdm(steps)\n",
        "    model.train()\n",
        "    for i, (x_batch, y_batch) in pbar:\n",
        "        y_batch_pred = model(x_batch.to(device))\n",
        "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
        "        if i == len(steps) - 1:\n",
        "            model.eval()\n",
        "            pred = model(x_test)\n",
        "            vloss = loss_fn(pred, y_test)\n",
        "            if es(model, vloss):\n",
        "                done = True\n",
        "            pbar.set_description(\n",
        "                f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, {es.status}\"\n",
        "            )\n",
        "        else:\n",
        "            pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNEqUJb8yC_S",
        "outputId": "afcfcc7e-c5c4-447c-e3a2-e0afb50d2ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 0.07578587532043457, vloss: 0.084684, : 100%|██████████| 469/469 [00:18<00:00, 25.13it/s]\n",
            "Epoch: 2, tloss: 0.4055657386779785, vloss: 0.084303, Improvement found, counter reset to 0: 100%|██████████| 469/469 [00:06<00:00, 77.10it/s] \n",
            "Epoch: 3, tloss: 0.07186264544725418, vloss: 0.086865, No improvement in the last 1 epochs: 100%|██████████| 469/469 [00:03<00:00, 117.81it/s]\n",
            "Epoch: 4, tloss: 0.0284480731934309, vloss: 0.083243, Improvement found, counter reset to 0: 100%|██████████| 469/469 [00:04<00:00, 101.33it/s]\n",
            "Epoch: 5, tloss: 0.009286901913583279, vloss: 0.087126, No improvement in the last 1 epochs: 100%|██████████| 469/469 [00:04<00:00, 110.33it/s]\n",
            "Epoch: 6, tloss: 0.254237562417984, vloss: 0.080455, Improvement found, counter reset to 0: 100%|██████████| 469/469 [00:03<00:00, 123.72it/s]\n",
            "Epoch: 7, tloss: 0.30666109919548035, vloss: 0.081701, No improvement in the last 1 epochs: 100%|██████████| 469/469 [00:03<00:00, 121.01it/s]\n",
            "Epoch: 8, tloss: 0.19933708012104034, vloss: 0.083186, No improvement in the last 2 epochs: 100%|██████████| 469/469 [00:04<00:00, 102.69it/s]\n",
            "Epoch: 9, tloss: 0.023665284737944603, vloss: 0.087177, No improvement in the last 3 epochs: 100%|██████████| 469/469 [00:04<00:00, 101.85it/s]\n",
            "Epoch: 10, tloss: 0.03374714031815529, vloss: 0.080949, No improvement in the last 4 epochs: 100%|██████████| 469/469 [00:03<00:00, 120.17it/s]\n",
            "Epoch: 11, tloss: 0.015354972332715988, vloss: 0.084659, Early stopping triggered after 5 epochs.: 100%|██████████| 469/469 [00:03<00:00, 123.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did not use the total number of requested epochs. The neural network training stopped once the validation set no longer improved."
      ],
      "metadata": {
        "id": "u-k7kobi2JPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(x_test)\n",
        "vloss = loss_fn(pred, y_test)\n",
        "print(f\"Loss = {vloss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wIKCYQE0nto",
        "outputId": "911eadd6-7314-4f94-e8bd-25f46be6f632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 0.08045466244220734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred = model(x_test)\n",
        "_, predict_classes = torch.max(pred, 1)\n",
        "correct = accuracy_score(y_test.cpu(), predict_classes.cpu())\n",
        "print(f\"Accuracy: {correct}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVRwVjON0nxf",
        "outputId": "56411278-db33-4b96-b5c5-583b3f342089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Early Stopping with Regression"
      ],
      "metadata": {
        "id": "2R8alazd2VgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Read the MPG dataset.\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/yunssamfinance/DeepLearningInFinance/main/boston.csv\", na_values=[\"NA\", \"?\"]\n",
        ")\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[\n",
        "    [\n",
        "        \"CRIM\",\n",
        "        \"ZN\",\n",
        "        \"INDUS\",\n",
        "        \"CHAS\",\n",
        "        \"NOX\",\n",
        "        \"RM\",\n",
        "        \"AGE\",\n",
        "        \"DIS\",\n",
        "        \"RAD\",\n",
        "        \"TAX\",\n",
        "        \"PTRATIO\",\n",
        "        \"B\",\n",
        "        \"LSTAT\"\n",
        "    ]\n",
        "].values\n",
        "y = df[\"MEDV\"].values  # regression\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Numpy to Torch Tensor\n",
        "x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, device=device, dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, device=device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset_train = TensorDataset(x_train, y_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_test, y_test)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# Create model\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 25),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(25, 1)\n",
        ")\n",
        "\n",
        "model = torch.compile(model, backend=\"aot_eager\").to(device)\n",
        "\n",
        "# Define the loss function for regression\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "es = EarlyStopping()\n",
        "\n",
        "epoch = 0\n",
        "done = False\n",
        "while epoch < 1000 and not done:\n",
        "    epoch += 1\n",
        "    steps = list(enumerate(dataloader_train))\n",
        "    pbar = tqdm.tqdm(steps)\n",
        "    model.train()\n",
        "    for i, (x_batch, y_batch) in pbar:\n",
        "        y_batch_pred = model(x_batch).flatten()  #\n",
        "        loss = loss_fn(y_batch_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
        "        if i == len(steps) - 1:\n",
        "            model.eval()\n",
        "            pred = model(x_test).flatten()\n",
        "            vloss = loss_fn(pred, y_test)\n",
        "            if es(model, vloss):\n",
        "                done = True\n",
        "            pbar.set_description(\n",
        "                f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\"\n",
        "            )\n",
        "        else:\n",
        "            pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTVwN65L0n02",
        "outputId": "def155fb-45e1-4cda-8bdf-8632b7c5226b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 40.7878532409668, vloss: 58.127304, EStop:[]: 100%|██████████| 24/24 [00:00<00:00, 25.56it/s]\n",
            "Epoch: 2, tloss: 64.16509246826172, vloss: 51.466068, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 125.68it/s]\n",
            "Epoch: 3, tloss: 48.284423828125, vloss: 48.127918, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 126.43it/s]\n",
            "Epoch: 4, tloss: 27.726341247558594, vloss: 37.862492, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 120.76it/s]\n",
            "Epoch: 5, tloss: 48.02397155761719, vloss: 54.010014, EStop:[No improvement in the last 1 epochs]: 100%|██████████| 24/24 [00:00<00:00, 112.73it/s]\n",
            "Epoch: 6, tloss: 34.65900802612305, vloss: 38.356411, EStop:[No improvement in the last 2 epochs]: 100%|██████████| 24/24 [00:00<00:00, 129.67it/s]\n",
            "Epoch: 7, tloss: 100.47348022460938, vloss: 31.589128, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 122.02it/s]\n",
            "Epoch: 8, tloss: 67.170166015625, vloss: 36.970074, EStop:[No improvement in the last 1 epochs]: 100%|██████████| 24/24 [00:00<00:00, 96.19it/s]\n",
            "Epoch: 9, tloss: 42.334495544433594, vloss: 53.260628, EStop:[No improvement in the last 2 epochs]: 100%|██████████| 24/24 [00:00<00:00, 96.76it/s]\n",
            "Epoch: 10, tloss: 33.835304260253906, vloss: 27.169619, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 98.96it/s] \n",
            "Epoch: 11, tloss: 69.38015747070312, vloss: 38.173759, EStop:[No improvement in the last 1 epochs]: 100%|██████████| 24/24 [00:00<00:00, 105.90it/s]\n",
            "Epoch: 12, tloss: 37.589752197265625, vloss: 24.084360, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 104.16it/s]\n",
            "Epoch: 13, tloss: 22.237457275390625, vloss: 23.499302, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 99.01it/s]\n",
            "Epoch: 14, tloss: 24.638805389404297, vloss: 22.294340, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 114.45it/s]\n",
            "Epoch: 15, tloss: 29.58563232421875, vloss: 32.558533, EStop:[No improvement in the last 1 epochs]: 100%|██████████| 24/24 [00:00<00:00, 96.30it/s]\n",
            "Epoch: 16, tloss: 17.11236000061035, vloss: 25.650845, EStop:[No improvement in the last 2 epochs]: 100%|██████████| 24/24 [00:00<00:00, 109.30it/s]\n",
            "Epoch: 17, tloss: 89.31737518310547, vloss: 49.362732, EStop:[No improvement in the last 3 epochs]: 100%|██████████| 24/24 [00:00<00:00, 107.26it/s]\n",
            "Epoch: 18, tloss: 26.840648651123047, vloss: 25.206316, EStop:[No improvement in the last 4 epochs]: 100%|██████████| 24/24 [00:00<00:00, 103.76it/s]\n",
            "Epoch: 19, tloss: 39.1115837097168, vloss: 20.673878, EStop:[Improvement found, counter reset to 0]: 100%|██████████| 24/24 [00:00<00:00, 112.50it/s]\n",
            "Epoch: 20, tloss: 41.980525970458984, vloss: 34.055504, EStop:[No improvement in the last 1 epochs]: 100%|██████████| 24/24 [00:00<00:00, 107.60it/s]\n",
            "Epoch: 21, tloss: 38.787845611572266, vloss: 21.700232, EStop:[No improvement in the last 2 epochs]: 100%|██████████| 24/24 [00:00<00:00, 96.95it/s] \n",
            "Epoch: 22, tloss: 18.21392822265625, vloss: 23.208204, EStop:[No improvement in the last 3 epochs]: 100%|██████████| 24/24 [00:00<00:00, 90.58it/s]\n",
            "Epoch: 23, tloss: 9.079489707946777, vloss: 23.479227, EStop:[No improvement in the last 4 epochs]: 100%|██████████| 24/24 [00:00<00:00, 99.92it/s] \n",
            "Epoch: 24, tloss: 40.31128692626953, vloss: 36.956783, EStop:[Early stopping triggered after 5 epochs.]: 100%|██████████| 24/24 [00:00<00:00, 93.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the error."
      ],
      "metadata": {
        "id": "PEpTrDV824gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "pred = model(x_test)\n",
        "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), y_test))\n",
        "print(f\"Final score (RMSE): {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3lZ1XACyDCS",
        "outputId": "6ed15316-5a94-4ef2-a0fb-f7c78182f5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final score (RMSE): 4.546853542327881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading a PyTorch Neural Network\n",
        "\n",
        "Complex neural networks will take a long time to fit/train. It is helpful to be able to save these neural networks so that you can reload them later. A reloaded neural network will not require retraining. PyTorch usually saves neural networks as [pickle](https://wiki.python.org/moin/UsingPickle) files. The following code trains a neural network to predict Boston housing price and saves the model."
      ],
      "metadata": {
        "id": "98lc2aHK3BY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Read the MPG dataset.\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/yunssamfinance/DeepLearningInFinance/main/boston.csv\", na_values=[\"NA\", \"?\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Select features and target\n",
        "features = df[\n",
        "    [\n",
        "        \"CRIM\",\n",
        "        \"ZN\",\n",
        "        \"INDUS\",\n",
        "        \"CHAS\",\n",
        "        \"NOX\",\n",
        "        \"RM\",\n",
        "        \"AGE\",\n",
        "        \"DIS\",\n",
        "        \"RAD\",\n",
        "        \"TAX\",\n",
        "        \"PTRATIO\",\n",
        "        \"B\",\n",
        "        \"LSTAT\"\n",
        "    ]\n",
        "]\n",
        "target = df[\"MEDV\"]  # regression\n",
        "\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert Numpy to PyTorch tensors\n",
        "features_tensor = torch.tensor(\n",
        "    scaled_features, device=device, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target.values, device=device, dtype=torch.float32)\n",
        "\n",
        "# Convert to TensorDataset\n",
        "dataset = TensorDataset(features_tensor, target_tensor)\n",
        "\n",
        "# Convert to DataLoader\n",
        "data_loader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "# Define the neural network using nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(features_tensor.shape[1], 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 25),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(25, 1),\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function for regression\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train for 1000 epochs.\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    for batch_features, batch_target in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch_features).flatten()\n",
        "        loss = loss_fn(out, batch_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Display status every 100 epochs.\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, loss: {loss.item()}\")\n",
        "\n",
        "model.eval()\n",
        "pred = model(features_tensor)\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), target_tensor))\n",
        "print(f\"Before save score (RMSE): {score}\")\n",
        "torch.save(model, \"medv.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqVHddyA258F",
        "outputId": "ab78428a-b866-47b4-e15c-cbb207150a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 216.08139038085938\n",
            "Epoch 100, loss: 8.785181999206543\n",
            "Epoch 200, loss: 4.241296291351318\n",
            "Epoch 300, loss: 3.1135926246643066\n",
            "Epoch 400, loss: 3.286428689956665\n",
            "Epoch 500, loss: 3.1825332641601562\n",
            "Epoch 600, loss: 1.73397696018219\n",
            "Epoch 700, loss: 1.6065764427185059\n",
            "Epoch 800, loss: 1.547377347946167\n",
            "Epoch 900, loss: 1.5836694240570068\n",
            "Before save score (RMSE): 1.085649847984314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below sets up a neural network and reads the data (for predictions), but it does not clear the model directory or fit the neural network. The code loads the weights from the previous fit. Now we reload the network and perform another prediction. The RMSE should match the previous one exactly if we saved and reloaded the neural network correctly.\n"
      ],
      "metadata": {
        "id": "8Wg4tn5h312e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure RMSE error for loaded network.  RMSE is common for regression.\n",
        "model.eval()\n",
        "pred = model(features_tensor)\n",
        "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(), target_tensor))\n",
        "print(f\"Before save score (RMSE): {score}\")\n",
        "torch.save(model, \"medv.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRm6GL7725_o",
        "outputId": "1ffd2a04-cbbc-4707-847a-f2f6967487f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before save score (RMSE): 1.085649847984314\n"
          ]
        }
      ]
    }
  ]
}